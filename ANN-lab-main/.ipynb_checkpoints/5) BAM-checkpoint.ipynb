{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c0f65-65c0-4ba8-9ccb-c947d140edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python Program for Bidirectional Associative Memory with two pairs of vectors.\n",
    "import numpy as np\n",
    "\n",
    "class BAM:\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.weights = np.zeros((input_shape, output_shape))\n",
    "\n",
    "    def train(self, input_pattern, output_pattern):\n",
    "        for i in range(input_pattern.shape[0]):\n",
    "            x = input_pattern[i]\n",
    "            y = output_pattern[i]\n",
    "\n",
    "            self.weights += np.outer(x, y)\n",
    "\n",
    "    # Recall output patterns\n",
    "    def recall_X_Y(self, input_pattern):\n",
    "        return np.dot(self.weights, input_pattern)\n",
    "\n",
    "    # Recall input patterns\n",
    "    def recall_Y_X(self, output_pattern):\n",
    "        return np.dot(self.weights, output_pattern)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "bam = BAM(input_size, output_size)\n",
    "\n",
    "input_pattern = np.array([[1,-1], [-1,1]])\n",
    "output_pattern = np.array([[-1,1], [1,-1]])\n",
    "\n",
    "bam.train(input_pattern, output_pattern)\n",
    "\n",
    "bam.weights\n",
    "\n",
    "#Testing input to output\n",
    "input_pattern = np.array([1,-1])\n",
    "bam.recall_X_Y(input_pattern)\n",
    "\n",
    "output_pattern = np.array([1,-1])\n",
    "bam.recall_Y_X(output_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d879b1-51ab-4252-9226-b6d09c57e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining BAM Class:\n",
    "# python\n",
    "# Copy code\n",
    "# class BAM:\n",
    "# This line defines a Python class named BAM that encapsulates the functionality of a Bipolar Associative Memory.\n",
    "# Constructor Method:\n",
    "# python\n",
    "# Copy code\n",
    "# def __init__(self, input_shape, output_shape):\n",
    "#     self.weights = np.zeros((input_shape, output_shape))\n",
    "# The __init__ method serves as the constructor for the BAM class. It initializes the weight matrix (self.weights) with zeros. The dimensions of the weight matrix are determined by input_shape (number of input neurons) and output_shape (number of output neurons).\n",
    "# Training Method:\n",
    "# python\n",
    "# Copy code\n",
    "# def train(self, input_pattern, output_pattern):\n",
    "#     for i in range(input_pattern.shape[0]):\n",
    "#         x = input_pattern[i]\n",
    "#         y = output_pattern[i]\n",
    "#         self.weights += np.outer(x, y)\n",
    "# The train method takes input-output pattern pairs (input_pattern and output_pattern) and updates the weight matrix (self.weights). It iterates over each pattern pair and updates the weights using Hebbian learning, where the weights are increased if the corresponding input and output units are active together.\n",
    "# Recall Methods:\n",
    "# python\n",
    "# Copy code\n",
    "# def recall_X_Y(self, input_pattern):\n",
    "#     return np.dot(self.weights, input_pattern)\n",
    "\n",
    "# def recall_Y_X(self, output_pattern):\n",
    "#     return np.dot(self.weights, output_pattern)\n",
    "# These methods perform pattern recall. recall_X_Y recalls the associated output pattern given an input pattern, while recall_Y_X recalls the associated input pattern given an output pattern. They utilize matrix multiplication to compute the activations of output (or input) neurons based on the provided input (or output) pattern and the learned weights.\n",
    "# Creating BAM Instance:\n",
    "# python\n",
    "# Copy code\n",
    "# input_size = 2\n",
    "# output_size = 2\n",
    "# bam = BAM(input_size, output_size)\n",
    "# This code snippet creates an instance of the BAM class named bam with input size 2 and output size 2.\n",
    "# Training BAM:\n",
    "# python\n",
    "# Copy code\n",
    "# input_pattern = np.array([[1,-1], [-1,1]])\n",
    "# output_pattern = np.array([[-1,1], [1,-1]])\n",
    "# bam.train(input_pattern, output_pattern)\n",
    "# Here, we define input and output patterns as numpy arrays and train the bam instance using these patterns.\n",
    "# Testing Recall:\n",
    "# python\n",
    "# Copy code\n",
    "# input_pattern = np.array([1,-1])\n",
    "# bam.recall_X_Y(input_pattern)\n",
    "# This code tests the recall of output pattern given an input pattern. We provide an input pattern and call the recall_X_Y method to retrieve the associated output pattern.\n",
    "# python\n",
    "# Copy code\n",
    "# output_pattern = np.array([1,-1])\n",
    "# bam.recall_Y_X(output_pattern)\n",
    "# Similarly, this code tests the recall of input pattern given an output pattern. We provide an output pattern and call the recall_Y_X method to retrieve the associated input pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076092b-e440-40de-9072-ccc79829a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective:\n",
    "\n",
    "The objective of Bidirectional Associative Memory (BAM) is to create a system that can associate two sets of patterns, such that given one set (input), the system can recall the associated patterns from the other set (output), and vice versa. BAM aims to mimic the associative memory capabilities of the human brain, where memories are interconnected and retrieval can occur bidirectionally.\n",
    "\n",
    "Theory:\n",
    "\n",
    "BAM operates on the principle of Hebbian learning, which states that neurons that fire together, wire together. In BAM, patterns are stored as associations between input and output patterns in a weight matrix. During training, for each input-output pattern pair, the corresponding elements of the input and output patterns activate simultaneously, strengthening the connections (weights) between them.\n",
    "\n",
    "During recall, presenting an input pattern to the network activates the associated output pattern by computing the dot product of the input pattern with the weight matrix. Similarly, presenting an output pattern activates the associated input pattern. This bidirectional recall is made possible because of the symmetrical nature of the weight matrix, where both input-to-output and output-to-input associations are stored.\n",
    "\n",
    "Applications:\n",
    "\n",
    "Pattern Recognition: BAM can be used for pattern recognition tasks where patterns need to be associated bidirectionally. For example, in facial recognition systems, BAM can associate images of faces with corresponding names bidirectionally.\n",
    "Speech Recognition: BAM can be employed in speech recognition systems to associate spoken words with text representations bidirectionally, enabling efficient retrieval of text from speech and vice versa.\n",
    "Data Retrieval: BAM can be used in databases to associate queries with corresponding data records bidirectionally. This enables efficient retrieval of data based on queries and vice versa.\n",
    "Medical Diagnosis: In medical diagnosis systems, BAM can associate symptoms with corresponding diseases bidirectionally, facilitating quick diagnosis based on symptoms and vice versa.\n",
    "Robotics: BAM can be utilized in robotics for environment mapping and navigation. It can associate sensory inputs with corresponding actions bidirectionally, enabling robots to navigate environments efficiently.\n",
    "Algorithm:\n",
    "\n",
    "Initialization: Initialize the weight matrix with zeros.\n",
    "Training: For each input-output pattern pair:\n",
    "Activate the corresponding elements of the input and output patterns.\n",
    "Update the weight matrix by adding the outer product of the input and output patterns.\n",
    "Recall (Input to Output):\n",
    "Present the input pattern to the network.\n",
    "Compute the dot product of the input pattern with the weight matrix to recall the associated output pattern.\n",
    "Recall (Output to Input):\n",
    "Present the output pattern to the network.\n",
    "Compute the dot product of the output pattern with the weight matrix to recall the associated input pattern.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Theory:\n",
    "\n",
    "Think of BAM as a way for a computer system to remember and recall information in a way similar to how our brains associate different concepts or memories. In BAM, we teach the system to link two sets of patterns together, so that if we present one set, the system can recall the associated patterns from the other set, and vice versa.\n",
    "\n",
    "Hebbian Learning Principle:\n",
    "BAM relies on a principle called Hebbian learning, which says that when two neurons (or components) are active at the same time, the connection between them gets stronger. This principle mirrors how our brain strengthens connections between neurons when they are activated together, forming memories or associations.\n",
    "Pattern Association:\n",
    "BAM stores associations between input and output patterns in a weight matrix. Each element of this matrix represents the strength of the association between a particular input pattern element and a particular output pattern element.\n",
    "Training Process:\n",
    "During training, we provide the system with pairs of input-output patterns. For each pair, we activate the corresponding elements of the input and output patterns. Then, we update the weight matrix based on these activations. If an input pattern element and its corresponding output pattern element are both active, we increase the strength of their connection in the weight matrix.\n",
    "Recall Process:\n",
    "When we want to recall information, we provide either an input pattern or an output pattern to the system. If we give it an input pattern, it computes the dot product of that pattern with the weight matrix to retrieve the associated output pattern. Similarly, if we give it an output pattern, it retrieves the associated input pattern.\n",
    "Bidirectional Recall:\n",
    "The beauty of BAM lies in its bidirectional recall capability. This means that if we present an input pattern, it can recall the associated output pattern, and if we present an output pattern, it can recall the associated input pattern. This is possible because the weight matrix stores both types of associations symmetrically.\n",
    "Applications:\n",
    "\n",
    "Memory Systems:\n",
    "BAM can be used in computer memory systems to efficiently retrieve related information. For example, in a search engine, it can associate search queries with relevant web pages bidirectionally, enabling quick retrieval of relevant results.\n",
    "Neural Networks:\n",
    "In artificial neural networks, BAM can serve as a building block for more complex architectures. It can be used in autoassociative networks, where it helps in pattern completion and error correction tasks.\n",
    "Robotics and Automation:\n",
    "BAM can play a role in robotics and automation by enabling robots to associate sensory inputs with corresponding actions bidirectionally. This helps in tasks like object recognition, navigation, and decision-making.\n",
    "Medical Systems:\n",
    "Medical diagnosis systems can benefit from BAM by associating symptoms with diseases bidirectionally. This facilitates accurate diagnosis based on observed symptoms and vice versa.\n",
    "Data Retrieval and Recommendation Systems:\n",
    "BAM can be used in databases and recommendation systems to associate user queries or preferences with relevant data or recommendations bidirectionally, enhancing user experience and efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
