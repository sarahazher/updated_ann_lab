{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b2736-5d41-46fb-a9a7-de6659d93747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . With a suitable example demonstrate the perceptron learning law with its decision regions using \n",
    "# python. Give the output in graphical form\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_classification(\n",
    "        n_samples=100,\n",
    "        n_features=2, \n",
    "        n_informative=1,\n",
    "        n_redundant=0,\n",
    "        n_classes=2, \n",
    "        n_clusters_per_class=1, \n",
    "        class_sep=10,\n",
    "        hypercube=False,\n",
    "        random_state=41)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "\n",
    "w = np.zeros(X.shape[1])\n",
    "b = 0\n",
    "\n",
    "w\n",
    "\n",
    "def perceptron(X, y, w, b, lr=0.1, epochs=200):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            z = np.dot(X[i], w) + b\n",
    "            if z > 0:\n",
    "                y_pred = 1\n",
    "            else:\n",
    "                y_pred = 0\n",
    "\n",
    "            if y_pred != y[i]:\n",
    "                w = w + lr * (y[i]-y_pred) * X[i]\n",
    "                b = b + lr * (y[i]-y_pred)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "w, b = perceptron(X, y, w, b)\n",
    "w, b\n",
    "\n",
    "if eqn Ax + By + c = 0, then this can be rearranged to calc m and c in y = mx+c\n",
    "m = -(A/B) and c = -(C/B) \n",
    "\n",
    "m = -(w[0]/w[1])\n",
    "b = -(b/w[1])\n",
    "\n",
    "x_in = np.linspace(-3, 3, 100)\n",
    "y_input = m*x_in + b\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_in, y_input, color='red')\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.ylim(-3,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563f465-4a36-4496-9a5a-afc23d99b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure, let's break down the code line by line:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# from sklearn.datasets import make_classification\n",
    "# This line imports the make_classification function from the sklearn.datasets module. This function is used to generate a synthetic dataset for classification tasks.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# These lines import the numpy library, used for numerical operations, and the matplotlib.pyplot module, used for plotting graphs.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# X, y = make_classification(\n",
    "#         n_samples=100,\n",
    "#         n_features=2, \n",
    "#         n_informative=1,\n",
    "#         n_redundant=0,\n",
    "#         n_classes=2, \n",
    "#         n_clusters_per_class=1, \n",
    "#         class_sep=10,\n",
    "#         hypercube=False,\n",
    "#         random_state=41)\n",
    "# This line generates a synthetic classification dataset using the make_classification function. It creates X as the feature matrix and y as the target vector.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# plt.scatter(X[:,0], X[:,1], c=y)\n",
    "# This line creates a scatter plot of the synthetic dataset X with the first feature on the x-axis and the second feature on the y-axis. The colors of the points are determined by the target vector y.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# w = np.zeros(X.shape[1])\n",
    "# b = 0\n",
    "# These lines initialize the weights w as a vector of zeros with the same number of elements as the number of features in X, and b as 0. These will be the parameters of the perceptron model.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# def perceptron(X, y, w, b, lr=0.1, epochs=200):\n",
    "# This line defines a function named perceptron which takes input features X, target labels y, weights w, bias b, learning rate lr, and number of epochs epochs.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "#     for epoch in range(epochs):\n",
    "#         for i in range(X.shape[0]):\n",
    "# These lines iterate over the dataset for a number of epochs. In each epoch, it iterates over each sample in the dataset.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "#             z = np.dot(X[i], w) + b\n",
    "# This line computes the weighted sum of the input features X[i] and the weights w, and adds the bias b to it. This represents the net input to the perceptron.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "#             if z > 0:\n",
    "#                 y_pred = 1\n",
    "#             else:\n",
    "#                 y_pred = 0\n",
    "# This line predicts the class label (y_pred) based on whether the net input z is greater than 0 or not.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "#             if y_pred != y[i]:\n",
    "#                 w = w + lr * (y[i]-y_pred) * X[i]\n",
    "#                 b = b + lr * (y[i]-y_pred)\n",
    "# This line updates the weights w and bias b based on the perceptron learning rule. If the prediction is incorrect (y_pred not equal to y[i]), the weights and bias are adjusted to reduce the error.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "#     return w, b\n",
    "# This line returns the updated weights w and bias b after training the perceptron.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# w, b = perceptron(X, y, w, b)\n",
    "# This line calls the perceptron function with the dataset X, target labels y, initial weights w, and bias b, and retrieves the updated weights w and bias b.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# m = -(w[0]/w[1])\n",
    "# b = -(b/w[1])\n",
    "# These lines calculate the slope m and y-intercept b of the decision boundary line based on the learned weights w and bias b.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# x_in = np.linspace(-3, 3, 100)\n",
    "# y_input = m*x_in + b\n",
    "# These lines create an array of 100 equally spaced points between -3 and 3 (x_in), and calculate the corresponding y-coordinates (y_input) on the decision boundary line using the equation of a line y = mx + b.\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# plt.figure()\n",
    "# plt.plot(x_in, y_input, color='red')\n",
    "# plt.scatter(X[:,0], X[:,1], c=y)\n",
    "# plt.ylim(-3,2)\n",
    "# These lines create a new figure, plot the decision boundary line (y_input) in red, scatter plot the dataset points (X[:,0], X[:,1]) with colors determined by the target labels y, and set the y-axis limits between -3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689802d-6057-4660-8f2f-220ddf1ec49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective of this demonstration is to illustrate the Perceptron Learning Law and its decision regions using Python. The Perceptron Learning Law is a rule that adjusts the weights of a perceptron in order to minimize errors in classification. By visualizing the decision regions of the perceptron, we can understand how it separates different classes in a dataset.\n",
    "\n",
    "# Theory:\n",
    "\n",
    "# A Perceptron is a basic building block of neural networks, mimicking the behavior of a single neuron. It takes multiple inputs, each multiplied by a weight, and produces a single binary output. The Perceptron Learning Law updates the weights iteratively based on the errors between the predicted outputs and the actual labels.\n",
    "\n",
    "# The decision regions of a perceptron refer to the boundaries in the input space that separate different classes. These boundaries are determined by the weights of the perceptron. Points on one side of a decision boundary are classified as belonging to one class, while points on the other side are classified as belonging to another class.\n",
    "\n",
    "# Applications:\n",
    "\n",
    "# Perceptrons and their learning algorithms have various applications in machine learning and artificial intelligence, including:\n",
    "\n",
    "# Binary classification tasks such as spam detection, sentiment analysis, and medical diagnosis.\n",
    "# Pattern recognition tasks like handwriting recognition, image classification, and object detection.\n",
    "# Control systems where decisions need to be made based on sensor inputs.\n",
    "# Robotics for tasks such as navigation, path planning, and object avoidance.\n",
    "# Algorithm:\n",
    "\n",
    "# The algorithm for demonstrating the Perceptron Learning Law and visualizing decision regions can be summarized as follows:\n",
    "\n",
    "# Initialize weights and bias.\n",
    "# Define training data with labeled examples belonging to two classes.\n",
    "# Train the perceptron using the Perceptron Learning Law, adjusting weights to minimize classification errors.\n",
    "# Visualize the decision regions by plotting the input space and coloring regions based on the perceptron's predictions.\n",
    "# Test the trained perceptron with new data points to observe its classification performance.\n",
    "# Example:\n",
    "\n",
    "# Let's consider a simple example where we have two classes of data points in a 2D space: red circles and blue squares. Our task is to train a perceptron to classify these points based on their coordinates. After training, we'll visualize the decision regions of the perceptron, which will separate the space into regions corresponding to each class.\n",
    "\n",
    "# Output in Graphical Form:\n",
    "\n",
    "# The output will consist of a plot showing the input space with the data points colored according to their true labels (red circles or blue squares), as well as the decision regions of the trained perceptron. The decision regions will be depicted as boundaries separating the space into regions where points are classified as belonging to different classes.\n",
    "\n",
    "# By visually inspecting the output, we can observe how the perceptron learns to classify the data points and how it divides the input space into distinct regions corresponding to each class. This demonstration provides a clear understanding of how the Perceptron Learning Law works and how perceptrons make decisions based on learned weights.\n",
    "\n",
    "# User\n",
    "# more theory info\n",
    "# ChatGPT\n",
    "# Certainly! Let's delve deeper into the theory behind the Perceptron Learning Law, decision regions, and their significance:\n",
    "\n",
    "# Perceptron Learning Law:\n",
    "\n",
    "# The Perceptron Learning Law, also known as the perceptron training algorithm or the perceptron rule, is a method for training a perceptron to classify input data into two classes. It operates by adjusting the weights of the perceptron based on errors made during classification. The key steps of the algorithm include:\n",
    "\n",
    "# Initialization: Initialize the weights and bias of the perceptron to small random values or zeros.\n",
    "# Iterative Training: Iterate through the training data points:\n",
    "# For each data point, compute the weighted sum of inputs and apply the activation function to obtain the predicted output.\n",
    "# Compare the predicted output with the actual label.\n",
    "# Update the weights and bias according to the perceptron learning rule to reduce the error.\n",
    "# Convergence: Repeat the training process until the perceptron converges, meaning it accurately classifies all training examples or reaches a predefined level of accuracy.\n",
    "# The Perceptron Learning Law is effective for linearly separable datasets, where the classes can be separated by a hyperplane in the input space. However, it may not converge if the data is not linearly separable.\n",
    "\n",
    "# Decision Regions:\n",
    "\n",
    "# Decision regions, also known as decision boundaries, are the regions in the input space where different classes are classified by the perceptron. These regions are determined by the weights and bias of the perceptron and represent the separation between classes. In a binary classification task, there are typically two decision regions: one for each class.\n",
    "\n",
    "# The decision regions of a perceptron are linear when the input features are linearly separable. This means the decision boundary is a hyperplane, such as a line in two dimensions or a plane in higher dimensions. The perceptron classifies points on one side of the decision boundary as belonging to one class and points on the other side as belonging to the other class.\n",
    "\n",
    "# Significance:\n",
    "\n",
    "# Understanding the Perceptron Learning Law and decision regions is crucial for grasping the capabilities and limitations of perceptrons and similar linear classifiers. Some key points to consider include:\n",
    "\n",
    "# Linear Separability: Perceptrons can only classify data that is linearly separable. If the data is not separable by a hyperplane, the perceptron learning algorithm may not converge.\n",
    "# Complexity: Perceptrons are simple models with linear decision boundaries. While this simplicity makes them easy to understand and implement, it also limits their ability to handle complex datasets with nonlinear relationships.\n",
    "# Building Blocks: Despite their limitations, perceptrons serve as foundational building blocks for more complex neural network architectures. By understanding how perceptrons learn and make decisions, we can appreciate the principles underlying neural network training and design.\n",
    "# Visualization:\n",
    "\n",
    "# Visualizing the decision regions of a perceptron provides intuitive insights into how it separates different classes in the input space. By plotting the decision boundary along with the data points, we can observe how the perceptron learns to classify points and how its decision regions evolve during training. This visualization helps in interpreting the behavior of the perceptron and assessing its performance on different datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
